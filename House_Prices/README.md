# House Price

**回归问题。**

**一、认识数据**

**①关注因变量（房价）的分布情况**

画直方图观察其分布，在本例中房价的分布是偏离正太分布，通过取对数的处理能得到标准的正态分布。

**②理解每个特征的含义，并分析它们对问题的重要性。**

建立一个表格，包含：**特征名**、**类型**（是数值还是类别）、**期望**（猜测其对房价的影响程度，按高、中、低分类）、**结论**（可视化数据，比如：画箱形图和回归图，看是否与猜测的一样，得到对房价影响最大的特征）

在本例中，分类特征：**OverallQual和YearBuilt**；数值特征：**TotalBsmtSF和GrLivArea**，这四个特征对房价的影响最大。

**③观察特征之间的关系及特征与因变量之间的关系**

1）计算**皮尔森相关系数矩阵**（注意：皮尔森相关系数是计算数值特征之间的线性相关性），并将矩阵进行可视化。观察哪些特征具有强相关性，并根据对房价影响的大小，可以去除其中对房价影响较小的，只保留下重要的特征。（**特征选择**）

2）单独输出与房价相关性最强的前K个特征的相关性矩阵。

3）单独输出与房价相关性最弱的前K个特征的相关性矩阵。

 

**二、预处理**

**1.数据清洗**

包括去重，异常点的处理。

 

**2.缺失值**

缺失值的处理需要分训练集和测试集，因为可能测试集中某些特征存在缺失值，而训练集中并没有缺失。一般先处理训练集中的缺失值，再处理测试集。

处理步骤：

①首先需要**观察缺失值的情况**，编写一个方法能够返回：**缺失的特征名，缺失个数，缺失值占样本的比例**。

②**处理缺失值**。各种处理方法参考Titanic文档（通常是采用前3种方法）。处理过程：

一般是**先删除缺失值较多的特征**（超过80%或90%），对于那些存在缺失值且与因变量相关性弱的特征也可以删除，同时对具有强相关性的多个特征，为防止特征冗余，也可删除对因变量相对影响小的，只保留影响最大的那个。

**分类特征和数值特征需要分别处理**：分类特征中取值本身存在”不存在“这种情况的（比如：GarageType，存在没有Garage的情况），当缺失时，可以采用方法2，将缺失作为一种取值，设为‘NA’，如果没有这种情况，一般是用众数填充；数值特征一般是用中位数（数据分布存在偏移）或者平均数填充，除非像MasVnrArea，其缺失是由于MasVnrType为NA，所以其面积用0来填充。

**注意**：对于描述同一个事物不同方面的多个特征（比如包含Bsmt的所有特征），在处理缺失值时，需要考虑各个特征之间的影响，最好一起处理。

 

**2.异常点**

检测异常点可以使用**可视化**方法，比如：画散点图，那些离群的且不符合常理的点为异常点；或者画箱形图，在上下界外的为异常点。

处理异常点方法：

**①直接删除**：如果异常的样本较少。

**②视为缺失值**：按缺失值方法处理。

**③不处理**：如果算法对异常点不敏感。

**注意**：一般是考虑对因变量有很大影响的特征，可视化这些重要的特征和输出的关系，如果数目较少，直接去除离群的点。

 

**3.数据变换**

调整**因变量**及**重要的特征**，使其分布**符合正态分布**，一般**右偏使用log(x)**，**左偏使用exp(x)**。如果特征在作完变换后仍不满足正态分布，可以像TotalBsmtSF一样处理，忽略有较多的0值，用中位数填充，再进行log变换，同时要能有有特征指明Bsmt是否存在，以防止0值忽略造成的信息丢失。

 

**4.特征选择**

进行特征选择的原因：

①避免**维度灾难**。

②移除与解决问题不相关的特征（**无关特征**）和**冗余特征**，提高学习效果。

进行特征选择的方法：

主要分为三类：

**过滤式**（filter）、**包裹式**（wrapper）、**嵌入式**（embedding）。

**1）过滤式**

过滤式是按照发散性或相关性对各个特征进行评分，设定阈值或选择阈值的个数，选择特征。

**①移除低方差的特征**

移除那些在所有样本中数值几乎都相同的特征，该方法只适合数值变量。比如对于取值只有0和1的布尔特征，如果超过0.95的样本取值都是0或1，则说明该特征的作用不大，可以去除。该方法一般是作为特征选择的初始步骤。在sklearn中的VarianceThreshold类，设置参数threshold，小于该阈值的特征会被删除。但对于分类特征也可以通过value_counts方法统计每个取值的样本数，若发现取值几乎相同的特征，可以进行删除，比如Utilities。

**②单变量特征选择**

衡量单个特征与输出变量之间的关系，根据设定的指标来衡量特征重要性，去除不重要的特征。该方法对理解数据有较好的效果，但对特征优化和提高泛化能力不一定有效。衡量相关性的方法：

**A）回归**：**皮尔森系数**，f_regression（单因素线性回归）。需要注意这两种都只对**线性关系**敏感，对非线性的，即使两个变量都具有一一对应的关系，皮尔森相关性可能也会接近0。所以通常需要可视化一下，确认相似性。计算皮尔森系数可以使用dataframe.corr()得到，其取值范围是【-1,1】，符号表示关系的正负，绝对值表示强度。

为了能表示特征和输出变量之间的非线性关系，可采用**基于树模型的特征排序**，为每个单独的特征和输出响应构建树的模型，但需注意过拟合的问题，所以树的深度需要控制，并运用交叉验证。

**B）分类**：**卡方检验chi2**和f_classif。

**2）包裹式**

根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

**①递归特征消除（RFE）**

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，会给每个特征指定一个权重，拥有最小绝对值权重的特征被踢出特征集，基于新的特征集进行下一轮训练，如此反复递归，直到剩余的特征数量达到所需的特征数。

**3）嵌入式**

先**使用某些机器学习的算法和模型**进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。能够对特征进行评分的模型有：回归模型，SVM，树模型。使用sklearn中的**SelectFromModel**能够将低于阈值threshold（需要调参）的特征移除，得到新的特征集。

**①基于L1的特征选择**

使用L1范数作为惩罚项的线性模型，能将不重要的特征的对应系数变为0，通过**SelectFromModel**来选择不为0的特征，适应的模型：linear_model.Lasso（回归）， linear_model.LogisticRegression 和svm.LinearSVC（分类）。

**②基于树的特征选择。**

常用的是随机森林，能够计算特征的重要程度，通过**SelectFromModel**设定的阈值来去除不相关的特征。

**注意**：一般在进行分类特征与数值特征间的转换前，会先进行一次特征选择，主要是通过**可视化**的方式或者利用**皮尔森系数**删除那些**与因变量相关性不强**的特征，也可以利用皮尔森相关性矩阵，删除那些**有强关联性的特征（特征冗余）**，只保留最重要的那个，同时可以删除那些具有低方差的特征。

在进行完分类特征和数值特征的转换后，可以使用**随机森林**模型对各个特征进行评价，设置阈值，选择与因变量相关的特征。

 

**5.分类特征转为数值特征**

**①有序的分类特征映射为有序的数值特征**

对于一些有大小或者优劣之分的特征，比如：BsmtQual，其取值是good,poor之类的，需要映射为有序的数值特征1,2,3…。使用dataframe的replace方法。

**②无序分类特征one-hot编码**

使用dataframe的get_dummies方法进行one-hot编码，进行one-hot编码时**训练集和测试集要一起**，防止最后特征空间的维度不一样。

 

**6.无量纲化**

一般是使用标准化，除了能达到无量纲化的要求，还能是每个特征的分布符合标准正态分布。使用sklearn中的StandardScaler()类。

 

**注意**：预处理过程中的每个步骤最好进行封装，编写为一个方法，因为测试集需要做与训练集一样的处理，封装成方法能够减少代码量，并且逻辑更清晰。

 

**三、模型构建**

如果特征和输出之间存在线性关系，一般优先考虑线性模型，还有在分类回归问题中应用较多的GBDT类的模型（比如Xgboost）。

在本例中，建立的模型有：

boosting模型：**GBDT，Xgoost和Lightgbm**，表现最好的为Xgboost

线性模型：**lasso回归、Ridge回归、ElasticNet和SVM**，表现最好的为ElasticNet

最后是使用bagging思想结合了Xgboost、ElasticNet和SVM这3种算法，通过调参给予不同的权重组合成的模型效果最好。

 

**四、算法评估**

回归问题一般是采用**均方差**（Mean Squared Error, MSE）或者**均方根差**(Root Mean SquaredError, RMSE)来评估模型，结合交叉验证取平均值。

 